
```markdown
# ğŸ§  Persian GPT-2 from Scratch: Snappfood Sentiment Generator

[![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=for-the-badge&logo=PyTorch&logoColor=white)](https://pytorch.org/)
[![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Transformers-yellow?style=for-the-badge)](https://huggingface.co/)
[![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=for-the-badge&logo=pandas&logoColor=white)](https://pandas.pydata.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg?style=for-the-badge)](https://opensource.org/licenses/MIT)

> **Implementation of a scaled-down GPT-2 model architecture from scratch using PyTorch, trained on Persian Snappfood comments for controllable sentiment text generation.**

---

## ğŸ“– Table of Contents
- [Overview](#-overview)
- [Key Features](#-key-features)
- [Model Architecture](#-model-architecture)
- [Dataset](#-dataset)
- [Installation & Usage](#-installation--usage)
- [Training Process](#-training-process)
- [Inference Examples](#-inference-examples)
- [Credits](#-credits)

---

## ğŸ“– Overview

This project implements the **GPT-2** architecture (Decoder-only Transformer) completely from scratch without using `GPT2LMHeadModel` or pre-built model classes from libraries. The primary goal is to demonstrate a deep understanding of Transformers, Causal Self-Attention, and Positional Embeddings.

The model is trained to generate **Persian comments** based on a control token (`[POS]` for positive or `[NEG]` for negative), allowing for conditional text generation.

---

## âœ¨ Key Features

* **From-Scratch Implementation**: Custom implementation of `CausalSelfAttention`, `MLP`, `Block`, and `GPT2` classes using pure PyTorch.
* **Nano-GPT Configuration**: A lightweight architecture designed for educational purposes and efficient training on limited GPU resources.
* **Advanced Tokenization**: Utilizes the **Meta-Llama 3.3** tokenizer, offering superior performance for the Persian language compared to standard GPT-2 tokenizers.
* **Controllable Generation**: The model accepts special tokens to steer the sentiment of the generated text.
* **Custom Training Loop**: Manual implementation of training epochs, validation steps, gradient clipping, and Cosine Annealing scheduling.

---

## ğŸ—ï¸ Model Architecture

Unlike the original massive GPT-2 (124M+ params), this project uses a "Nano" configuration suitable for academic experimentation:

| Hyperparameter | Value | Description |
| :--- | :--- | :--- |
| **Layers (Blocks)** | 3 | Number of Transformer blocks |
| **Embedding Dim** | 192 | Dimensionality of embeddings and hidden states |
| **Attention Heads** | 3 | Number of heads in Multi-Head Attention |
| **Context Window** | 128 | Maximum sequence length |
| **Dropout** | 0.1 | Regularization rate |
| **Vocab Size** | ~128k | Derived from Llama 3.3 Tokenizer |

*Note: Learnable Position Embeddings (`wpe`) are used instead of fixed sinusoidal encodings.*

---

## ğŸ“Š Dataset

The model is trained on the **Cleaned Snappfood Persian Sentiment Analysis** dataset.

* **Source**: [Kaggle - Snappfood Persian Sentiment Analysis](https://www.kaggle.com/datasets/mohammad1ziyar/cleaned-snappfood-persian-sentiment-analysis)
* **Content**: User reviews from the Snappfood delivery platform.
* **Labels**: Binary (0 for Negative, 1 for Positive).
* **Preprocessing**: 
    * Positve comments prepended with `[POS]`
    * Negative comments prepended with `[NEG]`

---

## ğŸš€ Installation & Usage

### 1. Prerequisites
Install the required dependencies:
```bash
pip install torch transformers pandas matplotlib tqdm kagglehub scikit-learn

```

### 2. Hugging Face Authentication

Since the Llama 3.3 tokenizer is a gated model, you must authenticate:

```python
from huggingface_hub import login
login(token="YOUR_HF_TOKEN")

```

### 3. Running Inference

You can generate text using the custom `generate` function provided in the notebook:

```python
# Generate a positive comment
pos_text = generate_comment(model, tokenizer, sentiment="Positive", max_length=30)
print(f"Generated: {pos_text}")

# Generate a negative comment
neg_text = generate_comment(model, tokenizer, sentiment="Negative", max_length=30)
print(f"Generated: {neg_text}")

```

---

## ğŸ“‰ Training Process

The model was trained for **15 epochs** on a T4 GPU.

* **Optimizer**: AdamW (`lr=5e-4`, `weight_decay=0.01`)
* **Scheduler**: CosineAnnealingLR
* **Loss Function**: CrossEntropyLoss
* **Batch Size**: 8

**Performance:**
The training loss converged consistently, demonstrating the model's ability to learn Persian syntax and sentiment patterns despite the small architecture size.

---

## ğŸ“ Inference Examples

Below are uncurated samples generated by the model after 15 epochs:

| Sentiment | Generated Text |
| --- | --- |
| **Positive** | `[POS] Ø®ÙˆØ¨ Ø¨ÙˆØ¯ Ú©ÛŒÙÛŒØªØ´ Ù…Ù†Ø§Ø³Ø¨ Ø¨ÙˆØ¯ Ùˆ Ø®ÛŒÙ„ÛŒ Ø³Ø±ÛŒØ¹ Ø¨Ù‡ Ø¯Ø³ØªÙ… Ø±Ø³ÛŒØ¯` |
| **Negative** | `[NEG] Ø§ÙØªØ¶Ø§Ø­ Ø¨ÙˆØ¯ Ù…Ù† Ù†Ù…ÛŒØ¯ÙˆÙ†Ù… Ú†Ø±Ø§ Ø´Ù…Ø§ Ú©Ù„Ø§ Ø³ÙØ§Ø±Ø´ Ù…ÛŒØ¯ÛŒÙ†` |
| **Positive** | `[POS] Ù…Ø«Ù„ Ù‡Ù…ÛŒØ´Ù‡ Ø¹Ø§Ù„ÛŒ Ø§Ø² Ù†Ø§ØªÙ„ÛŒ Ú©Ù‡ Ø®ÛŒÙ„ÛŒ Ø±Ø§Ø¶ÛŒÙ…` |
| **Negative** | `[NEG] Ø¬ÙˆØ¬Ù‡â€ŒÙ‡Ø§ Ú©ÛŒÙÛŒØª Ù…Ù†Ø§Ø³Ø¨ÛŒ Ù†Ø¯Ø§Ø´ØªÙ†Ø¯ Ùˆ Ø³ÙˆØ®ØªÙ‡ Ø¨ÙˆØ¯` |

---

## ğŸ“ Credits

* **Course**: Deep Learning (Spring 2025)
* **Institution**: Sharif University of Technology (SUT), Computer Engineering Department.
* **Assignment**: Homework 3 - Practical.
* **Student**: Yalda Karimi (40113012004).
* **Designer**: Shaygan Adim.

---

<div align="center">
<sub>Built with â¤ï¸ using PyTorch</sub>
</div>

```

```
